<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!--
   This file is managed by DataStax OpsCenter LifeCycle Manager.
   Manual edits will be overwritten by the next install or configure
   job that runs on this system.
-->

<configuration>

    <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
    <!-- that are implied by Hadoop setup variables.                                                -->
    <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
    <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
    <!-- resource).                                                                                 -->

    <!-- Hive Execution Parameters -->
    <property>
        <name>hive.exec.mode.local.auto</name>
        <value>{{hive-exec-mode-local-auto}}</value>
        <description>Let hive determine whether to run in local mode automatically</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <!-- Use dsefs file system, set it to  dsefs://127.0.0.1:5598/user/spark/warehouse -->
        <value>{{hive-metastore-warehouse-dir}}</value>
        <description>location of default database for the warehouse</description>
    </property>

    <property>
        <name>hive.metastore.rawstore.impl</name>
        <value>{{hive-metastore-rawstore-impl}}</value>
        <description>Use the Apache Cassandra Hive RawStore implementation</description>
    </property>

    <property>
      <name>com.datastax.bdp.fs.client.authentication.factory</name>
      <value>{{com-datastax-bdp-fs-client-authentication-factory}}</value>
    </property>

    <!-- Set this to true to enable auto-creation of Cassandra keyspaces as Hive Databases -->
    <property>
        <name>cassandra.autoCreateHiveSchema</name>
        <value>{{cassandra-auto-create-hive-schema}}</value>
    </property>

    <property>
        <name>spark.enable</name>
        <value>{{spark-enable}}</value>
    </property>

    <property>
        <name>cassandra.connection.metaStoreColumnFamilyName</name>
        <value>{{cassandra-connection-meta-store-column-family-name}}</value>
    </property>

    <property>
      <name>hive.server2.enable.doAs</name>
      <value>{{hive-server2-enable-do-as}}</value>
    </property>

    <!-- https://issues.apache.org/jira/browse/HIVE-8529 -->
    <property>
      <name>hive.server2.logging.operation.enabled</name>
      <value>{{hive-server2-logging-operation-enabled}}</value>
    </property>
    <property>
      <name>hive.server2.logging.operation.level</name>
      <value>{{hive-server2-logging-operation-level}}</value>
    </property>

    <!-- Spark SQL Thrift Server security configuration -->
    <!--
    <property>
      <name>hive.server2.authentication.kerberos.principal</name>
      <value>hiveserver2/_HOST@EXAMPLE.COM</value>
    </property>

    <property>
      <name>hive.server2.authentication.kerberos.keytab</name>
      <value>/path/to/hiveserver2.keytab</value>
    </property>

    <property>
      <name>hive.server2.authentication</name>
      <value>kerberos</value>
    </property>
    -->

    <property>
        <name>hive.server2.transport.mode</name>
        <value>{{hive-server2-transport-mode}}</value>
    </property>

    <property>
        <name>spark.hadoop.com.datastax.bdp.fs.client.authentication.factory</name>
        <value>{{spark-hadoop-com-datastax-bdp-fs-client-authentication-factory}}</value>
    </property>
</configuration>
