<?xml version="1.0"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<!--
   This file is managed by DataStax OpsCenter LifeCycle Manager.
   Manual edits will be overwritten by the next install or configure
   job that runs on this system.
-->

<configuration>

    <!-- Hive Configuration can either be stored in this file or in the hadoop configuration files  -->
    <!-- that are implied by Hadoop setup variables.                                                -->
    <!-- Aside from Hadoop setup variables - this file is provided as a convenience so that Hive    -->
    <!-- users do not have to edit hadoop configuration files (that may be managed as a centralized -->
    <!-- resource).                                                                                 -->

    <!-- Hive Execution Parameters -->
    <property>
        <name>hive.exec.mode.local.auto</name>
        <value>{{hive-exec-mode-local-auto}}</value>
        <description>Let hive determine whether to run in local mode automatically</description>
    </property>

    <property>
        <name>hive.metastore.warehouse.dir</name>
        <!-- Use dsefs file system, set it to  dsefs://127.0.0.1:5598/user/spark/warehouse -->
        <value>{{hive-metastore-warehouse-dir}}</value>
        <description>location of default database for the warehouse</description>
    </property>

    <property>
        <name>hive.metastore.rawstore.impl</name>
        <value>{{hive-metastore-rawstore-impl}}</value>
        <description>Use the Apache Cassandra Hive RawStore implementation</description>
    </property>

    <property>
      <name>com.datastax.bdp.fs.client.authentication.factory</name>
      <value>{{com-datastax-bdp-fs-client-authentication-factory}}</value>
    </property>

    <!-- Set this to true to enable auto-creation of Cassandra keyspaces as Hive Databases -->
    <property>
        <name>cassandra.autoCreateHiveSchema</name>
        <value>{{cassandra-auto-create-hive-schema}}</value>
    </property>

    <property>
        <name>spark.enable</name>
        <value>{{spark-enable}}</value>
    </property>

    <property>
        <name>cassandra.connection.metaStoreColumnFamilyName</name>
        <value>{{cassandra-connection-meta-store-column-family-name}}</value>
    </property>

    <!-- https://issues.apache.org/jira/browse/HIVE-8529 -->
    <property>
      <name>hive.server2.logging.operation.enabled</name>
      <value>{{hive-server2-logging-operation-enabled}}</value>
    </property>

    <property>
      <name>cassandra.port</name>
      <value>${cassandra.connection.native.port}</value>
    </property>

  <!--There are three possible JDBC authentication setups for Spark SQL Thrift Server.-->
  <!--Uncomment one section at a time to use the selected authentication setup.-->

  <!-- Start of: configuration for not authenticating JDBC users -->
  {% ifequal jdbc-auth-type "none" %}
  <property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
  </property>

  <property>
    <name>hive.server2.authentication</name>
    <value>NONE</value>
  </property>
  {% endifequal %}
  <!-- End of: configuration for not authenticating JDBC users -->

  <!-- Start of: configuration for authenticating JDBC users with Kerberos -->
  <!--
    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>

    <property>
      <name>hive.server2.authentication</name>
      <value>KERBEROS</value>
    </property>

    <property>
      <name>hive.server2.authentication.kerberos.principal</name>
      <value>hiveserver2/_HOST@EXAMPLE.COM</value>
    </property>

    <property>
      <name>hive.server2.authentication.kerberos.keytab</name>
      <value>/path/to/hiveserver2.keytab</value>
    </property>
  -->
  <!-- End of: configuration for authenticating JDBC users with Kerberos -->

  <!-- Start of: configuration for authenticating JDBC users with plain text password -->
  {% ifequal jdbc-auth-type "password" %}
    <property>
      <name>hive.server2.enable.doAs</name>
      <value>true</value>
    </property>

    <property>
      <name>hive.server2.authentication</name>
      <value>CUSTOM</value>
    </property>

    <property>
      <name>hive.server2.custom.authentication.class</name>
      <value>com.datastax.bdp.auth.DseThriftServerPlainTextAuthProvider</value>
    </property>
  {% endifequal %}
  <!-- End of: configuration for authenticating JDBC users with plain text password -->

  {% comment %}
  The following section is not present in the bdp repo. It comes from OPSC-14286.
  If client encryption is enabled in cassandra.yaml, this section becomes necessary
  for studio to be able to connect (it assumes that if CQL is using SSL, then so is
  SparkSQL).
  {% endcomment %}
  <!-- SSL options for SparkSQL -->
    <property>
        <name>hive.server2.use.SSL</name>
        <value>{{use-ssl}}</value>
    </property>
    {% if use-ssl %}
    <property>
        <name>hive.server2.keystore.path</name>
        <value>{{keystore}}</value>
    </property>
    <property>
        <name>hive.server2.keystore.password</name>
        <value>{{keystore-password}}</value>
    </property>
    {% endif %}

    <property>
        <name>hive.server2.transport.mode</name>
        <value>{{hive-server2-transport-mode}}</value>
    </property>

    <property>
        <name>spark.hadoop.com.datastax.bdp.fs.client.authentication.factory</name>
        <value>{{spark-hadoop-com-datastax-bdp-fs-client-authentication-factory}}</value>
    </property>
</configuration>
