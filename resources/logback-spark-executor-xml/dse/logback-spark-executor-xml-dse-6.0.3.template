<!--
   This file is managed by DataStax OpsCenter LifeCycle Manager.
   Manual edits will be overwritten by the next install or configure
   job that runs on this system.
-->

<!--
  ~ Copyright DataStax, Inc.
  ~
  ~ Please see the included license file for details.
  -->

<configuration scan="false">
    <appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
        <target>System.out</target>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>WARN</level>
            <onMatch>DENY</onMatch>
            <onMismatch>NEUTRAL</onMismatch>
        </filter>
        <filter class="ch.qos.logback.classic.filter.LevelFilter">
            <level>ERROR</level>
            <onMatch>DENY</onMatch>
            <onMismatch>NEUTRAL</onMismatch>
        </filter>
        <encoder>
            <pattern>%-5level %date{ISO8601} %c: %msg%n</pattern>
        </encoder>
    </appender>

    <appender name="STDERR" class="ch.qos.logback.core.ConsoleAppender">
        <target>System.err</target>
        <filter class="ch.qos.logback.classic.filter.ThresholdFilter">
            <level>WARN</level>
        </filter>
        <encoder>
            <pattern>%-5level %date{ISO8601} %F:%L - %c: %msg%n</pattern>
        </encoder>
    </appender>

    <root level="{{root-level}}">
        <appender-ref ref="STDOUT" />
        <appender-ref ref="STDERR" />
    </root>

    <logger name="com.datastax.driver.core.CodecRegistry" level="{{com-datastax-driver-core-CodecRegistry}}"/>
    <logger name="org.eclipse.jetty" level="{{logger-jetty-level}}"/>

     <!-- Settings to quiet third party logs that are too verbose -->
    <logger name="org.apache.spark.util.logging.FileAppender" level="{{org-apache-spark-util-logging-FileAppender}}"/>
    <logger name="org.spark_project.jetty" level="{{org-spark_project-jetty}}"/>
    <logger name="org.spark_project.jetty.util.component.AbstractLifeCycle" level="{{org-spark_project-jetty-util-component-AbstractLifeCycle}}"/>
    <logger name="org.apache.spark.repl.SparkIMain$exprTyper" level="{{org-apache-spark-repl-SparkIMain-exprTyper}}"/>
    <logger name="org.apache.spark.repl.SparkILoop$SparkILoopInterpreter" level="{{org-apache-spark-repl-SparkILoop-SparkILoopInterpreter}}"/>
    <logger name="org.apache.parquet" level="{{org-apache-parquet}}"/>
    <logger name="parquet" level="{{parquet}}"/>

    <!-- SPARK-9183: Settings to avoid annoying messages when looking up
        nonexistent UDFs in SparkSQL with Hive support -->
    <logger name="org.apache.hadoop.hive.metastore.RetryingHMSHandler" level="{{org-apache-hadoop-hive-metastore-RetryingHMSHandler}}"/>
    <logger name="org.apache.hadoop.hive.ql.exec.FunctionRegistry" level="{{org-apache-hadoop-hive-ql-exec-FunctionRegistry}}"/>
</configuration>
